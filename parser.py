"""The ShivyC parser. It's written entirely by hand because automatic parser
generators are no fun.

"""
from collections import namedtuple

import ast
from errors import ParserError
import errors
from tokens import Token
import token_kinds

class Parser:
    """Provides the parser functionality to convert a list of tokens into an
    AST.

    Each internal function expect_* corresponds to a unique non-terminal symbol
    in the C grammar. It parses self.tokens beginning at the given index to
    try to match a grammar rule that generates the desired symbol. If a match is
    found, it returns a tuple (Node, index) where Node is an AST node for that
    match and index is one more than that of the last token consumed in that
    parse. If no match is not found, returns (None, 0) and saves the potential
    error to the errors variable.

    tokens (List(Token)) - The list of tokens to be parsed
    best_error (ParserError) - The "best error" encountered thusfar. That is,
    out of all the errors encountered thusfar, the one that occurred after
    succesfully parsing the most tokens.
    """
    
    def __init__(self, tokens):
        self.tokens = tokens
        self.best_error = None

    def parse(self):
        """Parse the provided list of tokens into an abstract syntax tree (AST)

        tokens (List[Token]) - A list of the tokens, as generated by the lexer
        returns (Node) - The root node of the generated AST"""

        try:
            node, index = self.expect_main(0)
        except ParserError as e:
            self.log_error(e)
            raise self.best_error

        # Ensure there's no tokens left at after the main function
        if self.tokens[index:]:
            err = "unexpected token"
            raise ParserError(err, index, self.tokens, ParserError.AT)

        return node

    def expect_main(self, index):
        """Ex: int main() { return 4; } """

        kinds_before = [token_kinds.int_kw, token_kinds.main,
                        token_kinds.open_paren, token_kinds.close_paren,
                        token_kinds.open_brack]
        match_start = self.match_tokens(self.tokens[index:], kinds_before)
        if match_start:
            index += match_start
        else:
            err = "expected main function starting"
            raise ParserError(err, index, self.tokens, ParserError.AT)

        nodes = []
        while True:
            try:
                node, index = self.expect_statement(index)
                nodes.append(node)
                continue
            except ParserError as e:
                self.log_error(e)

            try:
                node, index = self.expect_declaration(index)
                nodes.append(node)
                continue
            except ParserError as e:
                self.log_error(e)
                # When all of our parsing attempts fail, break out of the loop
                break

        if self.match_token(self.tokens[index:], token_kinds.close_brack):
            index += 1
        else:
            err = "expected closing brace"
            raise ParserError(err, index, self.tokens, ParserError.GOT)
        
        return (ast.MainNode(nodes), index)

    def expect_statement(self, index):
        try:
            return self.expect_return(index)
        except ParserError as e:
            self.log_error(e)

        return self.expect_expr_statement(index)

    def expect_semicolon(self, node, index):
        """Expect a semicolon at tokens[index]. If one is found, returns
        (node, index + 1). Otherwise, returns (None, 0) and adds an error.
        """
        if self.match_token(self.tokens[index:], token_kinds.semicolon):
            return (node, index + 1)
        else:
            err = "expected semicolon"
            raise ParserError(err, index, self.tokens, ParserError.AFTER)
        
    def expect_return(self, index):
        if self.match_token(self.tokens[index:], token_kinds.return_kw):
            index += 1
        else:
            err = "expected return keyword"
            raise ParserError(err, index, self.tokens, ParserError.GOT)

        node, index = self.expect_expression(index)
        return self.expect_semicolon(ast.ReturnNode(node), index)

    def expect_expr_statement(self, index):
        """Try to parse an expression, and also expect a semicolon after what
        was parsed."""
        node, index = self.expect_expression(index)
        
        return self.expect_semicolon(ast.ExprStatementNode(node),
                                     index)
    
    def expect_expression(self, index):
        """Implemented as a shift-reduce parser. Tries to comprehend as much as
        possible of tokens past index as being an expression, and the index
        returned is the first token that could not be parsed into the
        expression. If literally none of it could be parsed as an expression,
        returns (None, 0) like usual.
        """
                
        # Dictionay of key-value pairs {TokenKind: precedence} where higher
        # precedence is higher.
        binary_operators = {token_kinds.plus: 11,
                            token_kinds.star: 12,
                            token_kinds.equals: 1}

        # The set of assignment_tokens (because these are right-associative)
        assignment_operators = {token_kinds.equals}

        # An item in the parsing stack. The item is either a Node or Token,
        # where the node must generate an expression, and the length is the
        # number of tokens consumed in generating this node.
        StackItem = namedtuple("StackItem", ['item', 'length'])
        stack = []

        # TODO: clean up  the if-statements here
        i = index
        while True:
            # If the top of the stack is a number, reduce it to an expression
            # node
            if (stack and isinstance(stack[-1].item, Token)
                and stack[-1].item.kind == token_kinds.number):
                stack[-1] = StackItem(ast.NumberNode(stack[-1].item), 1)
            
            # If the top of the stack is an identifier, reduce it to
            # an identifier node
            elif (stack and isinstance(stack[-1].item, Token)
                and stack[-1].item.kind == token_kinds.identifier):
                stack[-1] = StackItem(ast.IdentifierNode(stack[-1].item), 1)

            # If the top of the stack matches ( expr ), reduce it to a
            # ParenExpr node
            elif (len(stack) >= 3
                  and isinstance(stack[-1].item, Token)
                  and stack[-1].item.kind == token_kinds.close_paren
                  and isinstance(stack[-2].item, ast.Node)
                  and isinstance(stack[-3].item, Token)
                  and stack[-3].item.kind == token_kinds.open_paren):
                expr = stack[-2]
                
                del stack[-3:]
                stack.append(
                    StackItem(ast.ParenExprNode(expr.item), expr.length + 2))

            # If the top of the stack matches a binary operator, reduce it to an
            # expression node.
            elif (len(stack) >= 3
                  and isinstance(stack[-1].item, ast.Node)
                  and isinstance(stack[-2].item, Token)
                  and stack[-2].item.kind in binary_operators.keys()
                  and isinstance(stack[-3].item, ast.Node)

                  # Make sure next token is not higher precedence
                  and not (i < len(self.tokens)
                           and self.tokens[i].kind in binary_operators.keys()
                           and (binary_operators[self.tokens[i].kind] >
                                binary_operators[stack[-2].item.kind]))
                  
                  # Make sure this and next token are not both assignment
                  # tokens, because assignment tokens are right associative.
                  and not (i < len(self.tokens)
                           and stack[-2].item.kind in assignment_operators
                           and self.tokens[i].kind in assignment_operators)):
                left_expr = stack[-3]
                right_expr = stack[-1]
                operator = stack[-2]

                # Remove these last 3 elements
                del stack[-3:]
                stack.append(
                    StackItem(ast.BinaryOperatorNode(left_expr.item,
                                                     operator.item,
                                                     right_expr.item),
                              left_expr.length + operator.length +
                              right_expr.length))
            else:
                # If we're at the end of the token list, or we've reached a
                # token that can never appear in an expression, stop reading.
                # Note we must update this every time the parser is expanded to
                # accept more identifiers.
                if i == len(self.tokens): break
                elif (self.tokens[i].kind != token_kinds.number
                      and self.tokens[i].kind != token_kinds.identifier
                      and self.tokens[i].kind != token_kinds.open_paren
                      and self.tokens[i].kind != token_kinds.close_paren
                      and self.tokens[i].kind not in binary_operators.keys()):
                    break
                
                stack.append(StackItem(self.tokens[i], 1))
                i += 1

        if stack and isinstance(stack[0].item, ast.Node):
            return (stack[0].item, index + stack[0].length)
        else:
            err = "expected expression"
            raise ParserError(err, index, self.tokens, ParserError.GOT)
        
    def expect_declaration(self, index):
        if self.match_token(self.tokens[index:], token_kinds.int_kw):
            index += 1
        else:
            err = "expected type name"
            raise ParserError(err, index, self.tokens, ParserError.GOT)

        if self.match_token(self.tokens[index:], token_kinds.identifier):
            variable_name = self.tokens[index]
            index += 1
        else:
            err = "expected identifier"
            raise ParserError(err, index, self.tokens, ParserError.AFTER)

        return self.expect_semicolon(ast.DeclarationNode(variable_name),
                                     index)

    #
    # Utility functions for the parser
    #
    def match_token(self, tokens, kind_expected):
        return self.match_tokens(tokens, [kind_expected])
    
    def match_tokens(self, tokens, kinds_expected):
        """Checks if the provided tokens match the expected token kinds, in
        order. If the tokens all have the expected kind, returns the length of
        kinds_expected. Otherwise, returns 0.

        tokens (List[Token]) - A list of tokens
        expected (List[TokenKind, None]) - A list of token kinds to expect

        """
        if len(tokens) < len(kinds_expected): return False
        if all(kind == token.kind for kind, token
               in zip(kinds_expected, tokens)):
            return len(kinds_expected)
        else: return 0

    def log_error(self, error):
        """Log the error in the parser to be used for error reporting. If the
        provided error occurred after parsing no fewer tokens than
        best_error.amount_parsed, replace best_error with the provided error.
        
        error (ParserError) - The error encountered.
        """
        if (not self.best_error or
            error.amount_parsed >= self.best_error.amount_parsed):
            self.best_error = error
